---
layout: layout
title: Introduction
---
<%= partial "nav" %>
<div class="container u-full-width u-max-full-width">
  <div class="row section-background">
    <div class="three columns">&nbsp;</div>
    <div class="six columns">
      <img class='cctv-cover' src="/images/banner.jpg" alt="Phone ban">
      <br><br>
      <h4 class='bold center'>Gendering Surveillance: An Introduction</h4>
      <h5 class='center'>Anja Kovacs - February 2017</h5>
    </div>
    <div class="three columns">&nbsp;</div>
  </div>
</div>

<div class="container u-full-width u-max-full-width">
  <main>
    <div class="row">
      <div class="three columns">&nbsp;</div>
      <div class="six columns">
        <section class="case-study-content" id='case-study'>
        <p></p><p></p>
        <p>With our work on Gendering Surveillance, the Internet Democracy Project hopes to make more concrete the multifaceted ways in which widespread surveillance shapes, and harms, our lives.</p>
        <p>At the time of writing this, we start with presenting you with three case studies on this site, examining the intersection of gender and surveillance in a variety of situations. Over time, we hope to further add to these. In addition, we share in a fourth piece some of the theory that has guided our explorations - this provides the broader framework that ties all work on this site together.</p>
        <p>In this introduction, I explain in some more detail why we decided to look at surveillance from a gender perspective. Why gendering surveillance?</p>
        <p>Let’s take a detour.</p>
        <p></p>
        <h5 class='center bold'>Contents</h5>
        <div class='card' id='toc-card'>
        <ol>
          <li><a href="#why">The Fledgling Fight for the Right to Privacy</a></li>
          <li><a href="#how">‘If You’ve Got Nothing to Hide, You’ve Got Nothing to Fear’?</a></li>
          <li><a href="#lessons">Gendering Our Approach</a></li>
          <li><a href="#references">References</a></li>
        </ol>
        </div>
        <h5 class='center bold' id='why'>1. The Fledgling Fight for the Right to Privacy</h5>
        <p></p>
        <p>Our starting point? An observation: while both government and private surveillance practices seem to be expanding all the time, the fight against indiscriminate surveillance and for greater privacy protections is not making much headway in India.</p>
        <p>In fact, it seems to be going in the opposite direction. In July 2015, the government explicitly challenged in the Supreme Court that the right to privacy is a fundamental right in India. The Supreme Court was hearing a variety of petitions protesting the government’s move to make the Aadhaar (India’s Unique ID number) compulsory to avail of various benefits and schemes.</p>
        <p>Since 1975, a number of judges in the Supreme Court have read the right to privacy into Article 21 of India’s Constitution, which guarantees the right to liberty. But those were small benches, with only two or three judges, and with their interpretation, they seemed to contradict two earlier judgments, pronounced by larger benches (Bhatia 2015). When the government questioned, in 2015, whether people in India have a fundamental right to privacy at all, the Supreme Court agreed that there were tensions between these two sets of judgments and referred the issue to a five-judge bunch to resolve it. As of January 2017, that bench, which was to address a seemingly pressing constitutional matter, has not been constituted (Bhatia 2017).</p>
        <p>What is at issue here here is not so much the validity of the question raised, but, as Usha Ramanathan has pointed out,[^1] the time at which this is raised. Why does the government explicitly argue that we do NOT have a fundamental right to privacy now, after more than forty years have passed since that first judgment?</p>
        <p>Because in the digital age, in the age of Aadhaar and Digital India, of Facebook and Uber and Google, the right to privacy - or its absence - matter like never before.</p>
        <p></p>
        <h5 class='center bold' id='why'>2. ‘If You’ve Got Nothing to Hide, You’ve Got Nothing to Fear’?</h5> 
        <p></p>
        <p>A second observation: important as these concerns might be, in public discourse they continue to be marked mostly by their absence.</p>
        <p>With safety and security frequently highlighted as surveillance’s goals, one reason for this void seems to be that the harms of surveillance in the digital age continue to remain too abstract for many people. For most people, the costs they pay for government and corporate surveillance are simply not evident enough. ‘If you’ve got nothing to hide, you’ve got nothing to fear’, they continue to believe. And that is that.</p>
        <p>If you really have nothing to hide, that speaks rather poorly about the quality of your inner life - and I am not arguing this merely to be provocative. Experiencing doubt, uncertainty and shame is integral to every person’s growth as a human being; it is what makes us human. By making not having anything to hide a matter of pride, we dampen our growth as a society.</p>
        <p>But what is perhaps even more important to highlight is that such arguments also hide from view this: being comfortable with revealing things about yourself often requires privilege. If your own identity and background fits closely within dominant norms - say if you are upper middle class, Hindu upper caste, male and heterosexual in India - you stand far less to lose by revealing details about who you are than if you are a poor, dalit, lesbian woman.</p>
        <p>Whether or not you have anything to hide is becoming less and less important in the rhetoric around surveillance, though. Because increasingly the message that indiscriminate surveillance really is to<sup><a>[a]</a></sup> our benefit, is for our own good, seems to also be gaining ground. This is mostly as a consequence of the normalisation of big data driven surveillance. ‘Give us all your data and we will give you… something! For free!’ - that’s basically the adagio of many of the big Internet giants. And increasingly, the government is using a similar discourse:  ‘give us your data and we’ll give you government benefits’. Neither is particular interested in guaranteeing the safekeeping of our data. We just have to trust that it will not be misused.</p>
        <p>This new thrust of pro-surveillance arguments poses a particular challenge for human rights advocates in developing countries. By advocating for restrictions on surveillance, are we standing in the way of our country’s growth? In India, we see such arguments play out especially in the context of Aadhaar, where the tensions between privacy and big data are overwhelmingly posited as a concern of development. Yet in developing countries as much as anywhere else, we can not overlook that big data can also function as a surveillance mechanism, with all the harms that come with it. Moreover, contrary to what many people believe, anonymising the data does not really resolve these challenges: study upon study has shown that with as little as four fairly simple spatio-temporal data points, the large majority of entries in a database can be de-anonymised, even if the database contains data for hundreds of thousands, even millions of people (see e.g. de Montjoye et al. 2013). </p>
        <p></p>
        <h5 class='center bold' id='why'>3. Gendering Our Approach</h5>
        <p></p>
        <p>How then to illustrate how deep the harms of surveillance can run?</p>
        <p>By gendering surveillance, we can really bring home the harms of surveillance. Indeed, surveillance of women is a long-standing practice in our society as elsewhere - and one that women from all castes, classes and religions are too familiar with, even if it affects them differently. As Richa Kaul Padte has argued so poignantly (2014):</p>
        <a></a><a></a><blockquote><p class='quote'>The constant and rigorous emphasis placed on the female body in societies across the world tells us two things: One, our bodies are something that we should hide, and paradoxically two, our bodies are something that are constantly on display. The presence of surveillance cameras in public or private spaces – hidden or otherwise – encapsulates this dichotomy perfectly. […] When it comes to spaces that tend to be male-dominated, your crime is the presence of your body, and the camera is, by extension, justified in capturing what you are supposed to hide’.</p></blockquote>
        <p>This is the then the crux of why we chose to look at surveillance from a gender perspective: the aim of surveillance really is to control people, so that everyone adheres as closely as possible to the norm (whatever that norm is).  And for centuries, women’s experiences have provided a wealth of information on what happens to those who are surveilled yet deviate from the norm - online abuse targeted at women exemplifies this today again so very well. By gendering surveillance, as we hope the research on this site will illustrate, the depth and range of the harms of surveillance in the digital age, too, can, thus, really be brought to the fore. (And yes, even if you don’t identify as a woman, this does concern you - as one day, it may well be you who is, willingly or unwillingly, found to be violating the norm).</p>
        <p>But by gendering surveillance, we do not only get more vivid, easily-relatable illustrations of the harm that surveillance in the digital age entails. In turn, as we will also see elsewhere on this site in detail,[^2] gendering surveillance also reshapes the debate on surveillance, since it brings to the fore aspects and dimensions that have so far largely remained absent from debates on surveillance and security in India. Gendering surveillance, in other words, both deepens and sharpens the debate, allowing us to move it forward in new, and what we believe are profoundly empowering ways.</p>
        <p> </p>
        
          
         <h5 class='center bold' id='how'>2. How Gendering Surveillance? </h5>
        <p> </p>
        <p>Let’s look at a few more theoretical insights on surveillance to equip us to ask the right questions.</p>
        <p></p>
        <p>2.1. The Two Dimensions of Surveillance</p>
        <p></p>
        <p>In particular, what the debates around big data especially remind us of is that surveillance is done for two different reasons:</p>
        <p>The first one is to monitor what you have done or are currently doing - that’s what happens, for example, when law enforcement has a strong suspicion that you might be engaging in a criminal activity. This dimension is usually referred to when we discuss surveillance as a tool to maintain security. It is what most people have in mind when they say, ‘you have nothing to fear if you have nothing to hide’. It is also the dimension that most frequently gets the spotlight when we talk about human rights violations relating to surveillance - the Snowden revelations, for example, are all about violations with regard to this dimension of surveillance.</p>
        <p> Many democratic societies had, over the years, developed a fairly stable consensus as to in what circumstances and to what extent monitoring is acceptable, or not - and generally this agreement included that domestic scrutiny at least should be quite specific, both in terms of who and when (Lyon 2003). However, the Internet and digital technology have now shattered this consensus almost everywhere in the world. As governments engage more and more in mass surveillance, mostly merely because they can, human rights activists argue, among other things, that they are criminalising everyone. Even those who are not even suspect of a crime now fall under the gaze of the state and this is contrary to international human rights law. Since the Snowden revelations, this part of the debate has received a fair amount of attention.</p>
        <p>However, there is a second dimension to surveillance practices, one that doesn’t get as much attention by far: surveillance can also shape what you will do in the future. That is because surveillance can incentivise certain kinds of behaviour, and discourage others. In that sense, as Jasbir Puar has noted, surveillance is pre-emptive: it seeks to control now, so that it can avoid having to repress later (Puar in conversation with West 2014). And it’s also productive, because it actually makes people do certain things. Just think of how carefully many of us tweak how we are perceived online, for example - indeed, very few of us are immune to the disciplinary power of the collective cybergaze (’That picture? No, don’t put up that picture!?! I look horrible in it!!’). </p>
        <p>The disciplinary dimensions of surveillance - perhaps most eloquently expanded on by Michel Foucault already  in 1975 (1995) - pre-date the big data era; the fascist political systems of the twentieth century, for example, played amply on its potential, creating subjects who both disciplined themselves and closely watched and scrutinised (as well as reported to the authorities, all too often) others.</p>
        <p>But with big data, the possibilities of shaping people’s behaviour have come become ever more comprehensive. Perhaps you remember, for example, how Facebook noticed that all of us had started to share fewer personal updates, which are so important to Facebook’s business model, and then in June 2016, announced that it had tweaked its algorithm in an effort to get us to do this more again (Wagner 2016). That is the power that big data has to shape what we do.</p>
        <p> There are even more insidious examples, through what is known as ‘social sorting’, by sorting people ‘into categories, assigning worth or risk, in ways that have real effects on their life-chances’ (Lyon 2003: 1). For example, more and more large companies use personality tests to sort through the large number of job applications they receive, even though plenty of studies have shown that such tests are a highly unreliable indicator of job performance. Yet if you don’t ‘crack’ that test, there is no future for you in these businesses. In this way, opaque algorithms that use proxies for what they claim to measure and that have inadequate-to-no feedback loops increasingly end up deciding our fate at crucial junctures in our lives (O’Neil 2016).</p>
        <p> Moreover, surveillance might operate in a more and more dispersed manner in the digital age, but it has certainly not become more democratic: who receives discipline and punishment, who is deemed worthy of pleasure and intimacy remains distributed in deeply uneven manners (Puar in conversation with West 2014). This is perhaps even more so where surveillance takes the shape of social sorting: we aren’t even all subjected to the same gaze now, but differently gazed at depending on which box the algorithm has decided to slot us in. And where the assumptions on which these algorithms are based are biased, these ‘weapons of math destruction’, as Cathy O’Neil has called them, therefore frequently only further entrench inequalities (O’Neil 2016). </p>
        <p>There are three important points to take away, then, from this discussion about surveillance’s avatars in the digital age. First, surveillance does not only take the form of watching and screening bodies and identities. It is also about identifying, tracking, monitoring, tabulating, analysing data (Monahan 2009 and Puar in conversation with West 2014). To fully grasp the harms of surveillance in the digital age, this is crucial to realise. </p>
        <p>Second, this effect is also achieved by many systems that perhaps do not primarily have surveillance as a goal, such as the datagathering machines of the big Internet companies. The uses and effects of those machines are, however, nevertheless of a surveillant nature (Monahan 2009).</p>
        <p>And third, whether or not you are put under surveillance is less and less in your control. Earlier, not committing any crimes could get you a long way. But today, your mere presence in place, or simply having provided one or more data points, is enough to be brought into yet another network of control. Unless you stop participating in modern public life, it will be hard to escape surveillance right now.</p>
        <p>2.2. Where Does Gender Fit In?</p>
        <p>How does gender fit into surveillance in its dual dimensions? Torin Monahan’s (2009) conceptual categories of gender and technology design helps to get a sense of the different ways in which this is possible. Monohan argues that there are three ways in which technologies have gendered outcomes.</p>
        <p>The first one is through body discrimination, which happens when technology privileges a certain type of person over others, effectively treating everyone who does not fit the norm as deviant. As a consequence, the outcomes that these technologies trigger are far less predictable for some people than for others.</p>
        <p>For example, the whole-body imaging technologies that are now used in many airports around the world to screen passengers are often represented as objective and neutral - after all, for these technologies colour does not exist. Yet as the aim of using such technologies is precisely to police non-normative bodies, some are far more likely to be treated as a potential threat, and thus to singled out for a secondary screening, than others (Magnet and Rodgers 2012). People with bodily shapes that may render them deviant thus apparently include obese people, who supposedly could hide weapons ‘between folds of fat and ﬂesh’! (Jen Phillips quoted in Magnet and Rodgers 2012).</p>
        <p>The second way in which technologies have gendered outcomes is through context or use discrimination. As Monahan explained in earlier work:</p>
        <a></a><a></a><blockquote><p class='quote'>'Because technologies are underdetermined, meaning that they take on values from the context of their use, existing conditions of inequality inflect technologies and technological systems, reproducing unequal social orders' (Monahan 2005).</p></blockquote>
        <p>And so, when existing social relationships are already patriarchal, ‘then surveillance (and other) technologies tend to amplify those tensions and inequalities’(Monahan 2009).</p>
        <p>When it comes to whole body imaging technologies, that means that transgender people, for example, are also at a heightened risk of being treated as ‘deviant’ and subjected to additional screenings - and to possibly be outed in public without their consent as a consequence (Magnet and Rodgers 2012). The case studies we present on this site have many additional examples of context or use discrimination.</p>
        <p>The third form is through discrimination by abstraction, where we are reduced to data points in databases, disembodied and denuded from our social context, abstract representations of the world - or at least of what those who are in control of the data consider important in it. It is this that facilitates the control at a distance that is so typical of modern surveillance systems - just think of how central data gathering is to the development and management of so-called ‘smart’ cities, supposedly so much smoother and more efficient in their functioning than the cities most of us actually live in today.</p>
        <p>But all too often, people’s social inequalities and experiences are not adequately reflected in the data. The fact that in India, as elsewhere, gender-disaggregated data is unavailable in so many cases just makes that all too clear. But things may be even worse when your individual data record is compared with the averages in a database as many weapons of math destruction do: in those cases, the fact that the inequalities you are suffering from might not be purely individual but have a collective dimension can simply disappear from view. And as existing structural inequalities are invisibilised, they end up getting further entrenched (O’Neil 2016).</p>
        <p>The result is that we are sometimes treated as the sum of a few data points, that our data takes precedence over who we are as human beings - such as when a person is taken aside in an airport merely because of their name, their nationality, the colour of their skin.</p>
        <p></p>
        <h5 class='center bold' id='lessons'>3. Lessons to Learn: Moving Beyond Privacy towards Social Justice</h5>
        <p> </p>
        <p>When writing the case studies, we have not always highlighted these aspects explicitly or have not always put them in elaborate conclusions - in many ways, we hope that the stories speak for themselves. But here are then some of the points that we hope that these case studies will make amply clear:</p>
        <ul>
        <li>That surveillance is about relations of power and domination.</li>
        <li>That it almost always reinscribes existing power equations (though also that that needn’t necessarily be the case - sometimes marginalised people have successfully managed to reverse the gaze, or to appropriate a dual-use technology for their own benefit).</li>
        <li>That it is pre-emptive, productive as well as repressive, but that in its existing avatars, it aims in all of these dimensions to control, even do away with, those who ‘deviate’ from the norm.</li>
        </ul>
        <p>Moreover, by recounting stories of rural and urban women, poor women and privileged women, we also hope to have shown that nobody escapes. Women’s exact location in webs of power and subordination impacts profoundly the extent and nature of their experiences of surveillance. Yet there are important connections in how these experiences are produced.</p>
        <p>What we hope will also become amply clear is, as David Lyon has pointed out, that fighting back against surveillance is ‘not merely a matter of personal privacy but of social justice’.</p>
        <p>Only fighting for privacy protections, while an important starting point, is, then, not enough. Fighting back against the harms of surveillance also requires, for example, transparency and accountability on the part of those who develop and implement powerful technologies. It requires preventing, to the extent possible, possibilities for manipulation of data and algorithms. And it requires grounding technologies once again ‘in social context, embodiment, and place’ (Monahan 2009: 299). Technologies of control at a distance facilitate the naturalisation of inequalities. It is only by putting technologies back into social contexts, and the webs of power relations that underlie them,  that the promise of surveillance as empowering - which safety apps,for example, like to claim - can ever possibly be realised.</p>
        <p>By gendering surveillance, we, thus, do not only get more vivid, easily-relatable illustrations of the harm that surveillance in the digital age entails. Gendering surveillance also brings to the fore aspects and dimensions that were heretofore largely absent from surveillance and security debates. In that sense, taking a gender approach to surveillance, thus, also reshapes these debates and redefines the arguments and strategies that human rights defenders will need to deploy to fight back against the harms of surveillance.</p>
        <p>If gender has always already been surveilled, bringing it into the heart of the debate on surveillance in the digital age will allow us to move that debate forward in new and profoundly empowering ways.</p>
        <p></p>
        <p>Footnotes</p>
        <p></p>
        <p>[^1]: At the Digital Empowerment Foundation’s Digital Citizen Summit in Bangalore, on 11 November 2016.</p>
        <p>[^2]: Want more on why and how? Read ‘Reading Surveillance through a Gendered Lens: Some Theory.</p>  
        <p></p>
        <h5 class='center bold' id='references'>References</h5>
        <p>Bhatia, Gautam (2015). The right to privacy and the Supreme Court’s referral: Two constitutional questions. Indian Constitutional Law and Philosophy, 11 August, <a target='_blank' rel='noopener noreferrer' href="https://indconlawphil.wordpress.com/2015/08/11/the-right-to-privacy-and-the-supreme-courts-referral-two-constitutional-questions/">https://indconlawphil.wordpress.com/2015/08/11/the-right-to-privacy-and-the-supreme-courts-referral-two-constitutional-questions/.</a></p>
        <p>Bhatia, Gautam (2017). ‘O brave new world’: The Supreme Court’s evolving doctrine of constitutional evasion. Indian Constitutional Law and Philosophy, 6 January, <a target='_blank' rel='noopener noreferrer' href="https://indconlawphil.wordpress.com/2017/01/06/o-brave-new-world-the-supreme-courts-evolving-doctrine-of-constitutional-evasion/">https://indconlawphil.wordpress.com/2017/01/06/o-brave-new-world-the-supreme-courts-evolving-doctrine-of-constitutional-evasion/.</a></p>
        <p>de Montjoye, Yves-Alexandre, Cesar A. Hidalgo, Michel Verleysen and Vincent D. Blondel (2013). Unique in the Crowd: The Privacy Bounds of Human Mobility. Scientific Reports, 3, <a target='_blank' rel='noopener noreferrer' href="http://www.nature.com/articles/srep01376">http://www.nature.com/articles/srep01376.</a></p>
        <p>Foucault, Michel (1995). Discipline and Punish: The Birth of the Prison. New York: Vintage Books.</p>
        <p>Lyon, David (2003). Introduction. In David Lyon (Ed.), Surveillance as Social Sorting: Privacy, Risk and Digital Discrimination. London: Routledge.</p>
        <p> Magnet, Shoshana and Tara Rodgers (2011). Stripping for the State: Whole Body Imaging Technologies and the Surveillance of Othered Bodies. Feminist Media Studies, 12(1): 10-118. </p>
        <p>Monahan, Torin (2005). Globalisation, Technological Change, and Public Education. New York: Routledge.</p>
        <p>Monahan, Torin (2009). Dreams of Control at a Distance: Gender, Surveillance and Social Control. Cultural Studies ↔ Critical Methodologies, 9(2): 286-305.</p>
        <p>O’Neil, Cathy (2016). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. UK: Allen Lane.</p>
        <p>Padte, Richa Kaul (2014). The Not-so-strange Feeling that Someone’s Always Watching You. Richa Kaul Padte, 4 September, <a target='_blank' rel='noopener noreferrer' href="https://richakaulpadte.com/2014/09/04/the-not-so-strange-feeling-that-someones-always-watching-you/">https://richakaulpadte.com/2014/09/04/the-not-so-strange-feeling-that-someones-always-watching-you/.</a></p>
        <p>Wagner, Kurt (2016). Facebook Is Cutting Traffic to Publishers in Favor of User-Generated Content. Recode, 29 June, <a target='_blank' rel='noopener noreferrer' href="http://www.recode.net/2016/6/29/12053800/facebook-news-feed-algorithm-change-publisher-traffic">http://www.recode.net/2016/6/29/12053800/facebook-news-feed-algorithm-change-publisher-traffic.</a></p>
        <p>West, Lewis (2014). Jasbir Puar: Regimes of Surveillance. Cosmologics Magazine, 4 December, <a target='_blank' rel='noopener noreferrer' href="http://cosmologicsmagazine.com/jasbir-puar-regimes-of-surveillance/">http://cosmologicsmagazine.com/jasbir-puar-regimes-of-surveillance/.</a></p>
        </section>
      </div>
      <div class="three columns">&nbsp;</div>
    </div>
    <div class='row section-background'>
      <div class="row">
        <div class="three columns">&nbsp;</div>
        <div class="five columns center">
          <h4 class='read-next bold'>Read next</h4>
        </div>
        <div class="four columns">&nbsp;</div>
      </div>
      <section class="case-studies" id="case-studies">
        <div class="row">
          <div class="four columns tile-container">
            <%= partial 'phone_ban_card' %>
          </div>
          <div class="four columns tile-container">
            <%= partial 'safety_app_card' %>
          </div>
          <div class="four columns tile-container">
            <%= partial 'cctv_card' %>
          </div>

          <div class="two columns tile-container">&nbsp;</div>
        </div>
      </section>
    </div>
  </main>
</div>
